{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('ecog_is2s': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fe8054fe0736511d0a995e424bd42fab5ba13013efdf79ed2907f82c79967e8d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Test `prediction.py` model loading\n",
    "Michael Nolan   2021.01.15"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "import prediction # this is where the pt-l model code is\n",
    "\n",
    "import h5py\n",
    "import argparse\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a beefy config file! Stanardize this, for sure\n",
    "wandb.init(\n",
    "    config = {\n",
    "        # model-agnostic hyperparameters\n",
    "        'data_file_path': \"D:\\\\Users\\\\mickey\\\\Data\\\\datasets\\\\ecog\\\\goose_wireless\\\\gw_250\",\n",
    "        'batch_size': 1000,\n",
    "        'sequence_length': 50,\n",
    "        'data_suffix': 'ecog',\n",
    "        'objective_function': 'mse',\n",
    "        'learning_rate': 0.001,\n",
    "        'learning_rate_factor': 0.9,\n",
    "        'device': 'cuda',\n",
    "\n",
    "        # model-specific hyperparameters\n",
    "        'g_encoder_size': 10,\n",
    "        'c_encoder_size': 0,\n",
    "        'g_latent_size': 10,\n",
    "        'u_latent_size': 0,\n",
    "        'controller_size': 0,\n",
    "        'generator_size': 10,\n",
    "        'factor_size': 10,\n",
    "        'prior': {\n",
    "            'g0' : {\n",
    "                'mean' : {\n",
    "                    'value': 0.0, \n",
    "                    'learnable' : True\n",
    "                    },\n",
    "                'var'  : {\n",
    "                    'value': 0.1, \n",
    "                    'learnable' : True\n",
    "                    },\n",
    "                },\n",
    "            'u'  : {\n",
    "                'mean' : {\n",
    "                    'value': 0.0, \n",
    "                    'learnable' : False\n",
    "                    },\n",
    "                'var'  : {\n",
    "                    'value': 0.1, \n",
    "                    'learnable' : True\n",
    "                    },\n",
    "                'tau'  : {\n",
    "                    'value': 10, \n",
    "                    'learnable' : True\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        'clip_val': 2.0,\n",
    "        'max_norm': 5.0,\n",
    "        'do_normalize_factors': True,\n",
    "        'factor_bias': False,\n",
    "        'loss_weight_dict': {\n",
    "            'kl': {\n",
    "                'weight': 0.0,\n",
    "                'min': 0.0,\n",
    "                'max': 1.0,\n",
    "                'schedule_dur': 1600,\n",
    "                'schedule_start': 0,\n",
    "            },\n",
    "            'l2': {\n",
    "                'weight': 0.0,\n",
    "                'min': 0.0,\n",
    "                'max': 1.0,\n",
    "                'schedule_dur': 1600,\n",
    "                'schedule_start': 0.0,\n",
    "            },\n",
    "            'l2_con_scale': 0,\n",
    "            'l2_gen_scale': 2000,\n",
    "        },\n",
    "        'l2_gen_scale': 0.9,\n",
    "        'l2_con_scale': 0.9,\n",
    "        'dropout': 0.0,\n",
    "        },\n",
    "    mode=\"disabled\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LfadsModel_ECoG(prediction.Lfads):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.data_file_path         = config.data_file_path\n",
    "        self.batch_size             = config.batch_size # check this - set by wandb in hparam sweeps\n",
    "        self.seq_len                = config.sequence_length\n",
    "        self.data_suffix            = config.data_suffix\n",
    "        self.objective_function     = config.objective_function\n",
    "        self.prepare_data()\n",
    "        with h5py.File(self.data_file_path,'r') as hdf:\n",
    "            _, _, self.input_size = hdf[f'test_{self.data_suffix}'].shape\n",
    "        self.generator_size         = config.generator_size\n",
    "        self.g_encoder_size         = config.g_encoder_size\n",
    "        self.g_latent_size          = config.g_latent_size\n",
    "        self.controller_size        = config.controller_size\n",
    "        self.c_encoder_size         = config.c_encoder_size\n",
    "        self.u_latent_size          = config.u_latent_size\n",
    "        self.factor_size            = config.factor_size\n",
    "\n",
    "        self.prior                  = config.prior\n",
    "\n",
    "        self.clip_val               = config.clip_val\n",
    "        self.factor_bias            = config.factor_bias\n",
    "\n",
    "        # is it poor form to put this at the end of the init call?\n",
    "        super(LfadsModel_ECoG, self).__init__(config)\n",
    "\n",
    "        # # create modules\n",
    "        # self.encoder = LFADS_Encoder(\n",
    "        #     self.input_size, \n",
    "        #     self.g_encoder_size, \n",
    "        #     self.g_latent_size, \n",
    "        #     c_encoder_size = self.c_encoder_size, \n",
    "        #     dropout = self.dropout, \n",
    "        #     clip_val = self.clip_val\n",
    "        #     )\n",
    "        # self.controller = LFADS_ControllerCell(\n",
    "        #     self.input_size, \n",
    "        #     self.controller_size, \n",
    "        #     self.u_latent_size, \n",
    "        #     dropout = self.dropout, \n",
    "        #     clip_val = self.clip_val, \n",
    "        #     factor_bias=self.factor_bias\n",
    "        #     )\n",
    "        # self.generator = LFADS_GeneratorCell(\n",
    "        #     input_size, \n",
    "        #     generator_size, \n",
    "        #     factor_size,\n",
    "        #     attention = False,\n",
    "        #     dropout=self.dropout, \n",
    "        #     clip_val=self.clip_val, \n",
    "        #     factor_bias=self.factor_bias\n",
    "        #     )\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # load datasets from hdf5 volume\n",
    "        data_dict = self.read_h5(self.data_file_path)\n",
    "        self.train_dataset  = EcogSrcTrgDataset(\n",
    "            data_dict[f\"train_{self.data_suffix}\"],\n",
    "            self.seq_len\n",
    "            )\n",
    "        self.valid_dataset  = EcogSrcTrgDataset(\n",
    "            data_dict[f\"valid_{self.data_suffix}\"],\n",
    "            self.seq_len\n",
    "            )\n",
    "        self.test_dataset   = EcogSrcTrgDataset(\n",
    "            data_dict[f\"test_{self.data_suffix}\"],\n",
    "            self.seq_len\n",
    "            )\n",
    "\n",
    "    # these are defined adequately in the prediction.Lfads class!\n",
    "    # def training_step(self,train_batch,batch_idx):\n",
    "    #     src, trg = train_batch\n",
    "    #     recon, (factors, gen_inputs) = self.forward(src, trg)\n",
    "    #     loss = self.loss()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_dataset,batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset,batch_size=self.batch_size)\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_h5(data_file_path):\n",
    "        try:\n",
    "            with h5py.File(data_file_path, 'r') as hf:\n",
    "                data_dict = {k: torch.tensor(hf[k].value) for k in hf.keys()}\n",
    "            return data_dict\n",
    "        except IOError:\n",
    "            print(f'Cannot open data file {data_file_path}.')\n",
    "            raise\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_model_specific_arguments(parent_parser):\n",
    "        parser = super(LFADS)\n",
    "        parser = argparse.ArgumentParser(parents=[parent_parser],add_help=False)\n",
    "        parser.add_argument() # add this for all LFADS hyperparameters! (like the obj. function)\n",
    "\n",
    "class EcogSrcTrgDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tensor, seq_len):\n",
    "        assert tensor.shape[1] >= 2*seq_len, f\"sequence length cannot be longer than 1/2 data sample length ({tensor.shape[1]})\"\n",
    "        self.tensor = torch.tensor(tensor).float()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src = self.tensor[index,:self.seq_len,:]\n",
    "        trg = self.tensor[index,self.seq_len:2*self.seq_len,:]\n",
    "        return (src, trg)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensor.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "prediction_model_shell = LfadsModel_ECoG(wandb.config)\n",
    "prediction_model_shell.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    name='LFADS-wandbtest',\n",
    "    project='GW_ECoG-Prediction'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor = 'avg_valid_loss',\n",
    "    dirpath = 'D:\\\\Users\\\\mickey\\\\Data\\\\models\\\\pytorch-lightning\\\\',\n",
    "    filename = 'lfads-{epoch:03d}-{val_loss:.3f}',\n",
    ")\n",
    "early_stopping_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor ='avg_valid_loss'\n",
    ")\n",
    "trainer = pl.Trainer(max_epochs=100, \n",
    "                    logger = wandb_logger, \n",
    "                    gpus=1, \n",
    "                    callbacks=[checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(prediction_model_shell)"
   ]
  }
 ]
}