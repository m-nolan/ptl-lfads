{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ecog_data\n",
    "import prediction\n",
    "import torch\n",
    "\n",
    "import wandb\n",
    "import pytorch_lightning as ptl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.tuner.batch_size_scaling import scale_batch_size\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'lr': 5e-4,\n",
    "    'lr_factor': 0.5,\n",
    "    'src_len': 50,\n",
    "    'trg_len': 50,\n",
    "    'batch_size': 1000,\n",
    "    'encoder_size': 256,\n",
    "    'encoder_layers': 1,\n",
    "    'generator_size': 256,\n",
    "    'generator_layers': 1,\n",
    "    'factor_size': 64,\n",
    "    'loss_weight_dict': {\n",
    "        'ayy': 'lmao',\n",
    "    },\n",
    "    'dropout': 0.3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure wandb\n",
    "wandb.init(\n",
    "    config = config_dict,\n",
    "    mode = 'disabled'\n",
    ")\n",
    "wandb_logger = WandbLogger(name='ah-jeez',project='lfads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure data module\n",
    "ldm = ecog_data.GooseWireless250(\n",
    "    wandb.config.src_len,\n",
    "    wandb.config.trg_len,\n",
    "    wandb.config.batch_size\n",
    ")\n",
    "ldm.prepare_data()\n",
    "ldm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prediction.Lfads(\n",
    "    src_size            = ldm.size()[-1],\n",
    "    encoder_size        = wandb.config.encoder_size,\n",
    "    encoder_layers      = wandb.config.encoder_layers,\n",
    "    generator_size      = wandb.config.generator_size,\n",
    "    generator_layers    = wandb.config.generator_layers,\n",
    "    factor_size         = wandb.config.factor_size,\n",
    "    loss_weight_dict    = wandb.config.loss_weight_dict,\n",
    "    dropout             = wandb.config.dropout,\n",
    "    learning_rate       = wandb.config.lr,\n",
    "    lr_factor           = wandb.config.lr_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ptl.Trainer(max_epochs=100, logger=wandb_logger, gpus=1)\n",
    "trainer.tune(model,ldm)\n",
    "scale_batch_size(trainer,model,init_val=1024,max_trials=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, trg = iter(ldm.train_dataloader()).__next__()\n",
    "pred_dict = model(src,trg)"
   ]
  }
 ]
}