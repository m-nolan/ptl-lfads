{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('ecog_is2s': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fe8054fe0736511d0a995e424bd42fab5ba13013efdf79ed2907f82c79967e8d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningMNISTClassifier(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LightningMNISTClassifier, self).__init__()\n",
    "\n",
    "        # MNIST samples are [1, 28, 28]\n",
    "        self.layer_1 = nn.Linear(28 * 28, 128)\n",
    "        self.layer_2 = nn.Linear(128, 256)\n",
    "        self.layer_3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        \n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_3(x)\n",
    "        x = torch.log_softmax(x, dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return nn.functional.nll_loss(logits, labels)\n",
    "\n",
    "    # def accuracy(self, logits, labels):\n",
    "    #     return nn.functional.\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        self.logger.experiment.log({'train_loss': loss})\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, valid_batch, batch_idx):\n",
    "        x, y = valid_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        self.logger.experiment.log({'valid_loss': loss})\n",
    "\n",
    "        return {'valid_loss': loss}\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        x, y = test_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        acc = self.accuracy\n",
    "\n",
    "    # This is called at the end of each validation epoch. Why?\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['valid_loss'] for x in outputs]).mean()\n",
    "        self.logger.experiment.log({'avg_valid_loss': avg_loss})\n",
    "\n",
    "        return {'avg_valid_loss': avg_loss}\n",
    "\n",
    "    def prepare_data(self):\n",
    "        MNIST(os.getcwd(), train=True, download=True)\n",
    "        MNIST(os.getcwd(), train=False, download=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        mnist_train = MNIST(os.getcwd(), train=True, download=False, transform=transform)\n",
    "        self.mnist_train, self.mnist_valid = random_split(mnist_train, [55000, 5000])\n",
    "        mnist_train = DataLoader(self.mnist_train, batch_size=1000)\n",
    "        return mnist_train\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        mnist_valid = DataLoader(self.mnist_valid, batch_size=1000)\n",
    "        return mnist_valid\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        mnist_test = MNIST(os.getcwd(), train=False, batch_size=1000)\n",
    "        mnist_test = DataLoader(mnist_test, batch_size=1000)\n",
    "        return mnist_test\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), 0.001)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(name='MNIST-32-bit-adam-0.001',project='MNIST-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(name='MNIST-32-bit-adam-0.001',project='MNIST-test')"
   ]
  },
  {
   "source": [
    "wandb.config = {\n",
    "    'input_shape': (1, 28, 28),\n",
    "    'layer_1_size': 128,\n",
    "    'layer_2_size': 256,\n",
    "    'layer_3_size': 10,\n",
    "}\n",
    "model = LightningMNISTClassifier()\n",
    "model.train_dataloader()\n",
    "# callbacks\n",
    "ckpt_cb = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='avg_valid_loss',\n",
    "    dirpath='c:\\\\Users\\\\mickey\\\\dev\\\\pytorch-lightning',\n",
    "    filename='mnist-{epoch:03d}-{val_loss:.3f}',\n",
    ")\n",
    "es_cb = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor='avg_valid_loss',\n",
    "\n",
    ")\n",
    "trainer = pl.Trainer(max_epochs=100, logger = wandb_logger, gpus=1, callbacks=[ckpt_cb, es_cb])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  }
 ]
}